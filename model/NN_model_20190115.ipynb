{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import gc\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import mean_squared_error, roc_auc_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(4590)\n",
    "\n",
    "pd.options.display.max_rows = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"../input/20181216_train.csv\")\n",
    "df_test = pd.read_csv(\"../input/20181216_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_new_feature(path):\n",
    "    df_new_feature = pd.read_csv(path)\n",
    "    \n",
    "    df_tr = pd.merge(df_train, df_new_feature, on=\"card_id\", how=\"left\")\n",
    "    df_te = pd.merge(df_test, df_new_feature, on=\"card_id\", how=\"left\")\n",
    "    \n",
    "    return df_tr, df_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_train, df_test = merge_new_feature(\"../input/merchants_nmf.csv\")\n",
    "df_train, df_test = merge_new_feature(\"../input/new_merchants_nmf.csv\")\n",
    "df_train, df_test = merge_new_feature(\"../input/prosperity_merchants_latest.csv\")\n",
    "df_train, df_test = merge_new_feature(\"../input/merchants_numerical_features.csv\")\n",
    "df_train, df_test = merge_new_feature(\"../input/merchants_item_scale_feature.csv\")\n",
    "df_train, df_test = merge_new_feature(\"../input/purchase_pattern.csv\")\n",
    "df_train, df_test = merge_new_feature(\"../input/purchase_pettern_nmf.csv\")\n",
    "df_train, df_test = merge_new_feature(\"../input/purchase_amount_pettern_nmf.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"Null_count\"] = df_train.isnull().sum(axis=1)\n",
    "df_test[\"Null_count\"] = df_test.isnull().sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dont_use = [\n",
    "    'card_id', \n",
    "    'first_active_month',\n",
    "    'second_active_date',\n",
    "    'diff_first_and_second',\n",
    "    'target','outliers', \n",
    "    'category_1_Y_std', \n",
    "    'hist_purchase_date_max',\n",
    "    'hist_purchase_date_min',\n",
    "    'hist_category_2_mean_mean',\n",
    "    'hist_category_3_mean_mean'\n",
    "]\n",
    "\n",
    "df_train_columns = [c for c in df_train.columns if c not in dont_use]\n",
    "target = df_train['target']\n",
    "target_outlier = df_train[\"outliers\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df_train_columns: \n",
    "    df_train[col], df_test[col] = df_train[col].replace(np.inf, np.nan).fillna(0.0), df_test[col].replace(np.inf, np.nan).fillna(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_each_col = np.max(df_train[df_train_columns].values, axis=0)\n",
    "\n",
    "for i, max_ in enumerate(max_each_col):\n",
    "    if max_ >= 1e+7:\n",
    "        df_train[df_train_columns[i]] = 1/(1+np.exp(-1*df_train[df_train_columns[i]].values))\n",
    "        df_test[df_train_columns[i]] = 1/(1+np.exp(-1*df_test[df_train_columns[i]].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "249"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_train_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLPNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(len(df_train_columns), 300)\n",
    "        self.fc2 = nn.Linear(300, 300)\n",
    "        self.fc3 = nn.Linear(300, 1)\n",
    "        self.dropout1 = nn.Dropout2d(0.2)\n",
    "        self.dropout2 = nn.Dropout2d(0.2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "#         x = F.relu(self.fc1(x))\n",
    "#         x = self.dropout1(x)\n",
    "#         x = F.relu(self.fc2(x))\n",
    "#         x = self.dropout2(x)\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.dropout2(x)\n",
    "        return self.fc3(x)\n",
    "    \n",
    "device = 'cpu'\n",
    "net = MLPNet().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 500\n",
    "num_epochs = 10\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=learning_rate)#, momentum=0.9, weight_decay=5e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPNet(\n",
       "  (fc1): Linear(in_features=249, out_features=300, bias=True)\n",
       "  (fc2): Linear(in_features=300, out_features=300, bias=True)\n",
       "  (fc3): Linear(in_features=300, out_features=1, bias=True)\n",
       "  (dropout1): Dropout2d(p=0.2)\n",
       "  (dropout2): Dropout2d(p=0.2)\n",
       ")"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float32').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-97-2f2908138f08>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0mx_train_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train_var\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0mx_val_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_val_var\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mtrain_rmse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean_squared_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0mval_rmse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean_squared_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_val_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.6/lib/python3.6/site-packages/sklearn/metrics/regression.py\u001b[0m in \u001b[0;36mmean_squared_error\u001b[0;34m(y_true, y_pred, sample_weight, multioutput)\u001b[0m\n\u001b[1;32m    237\u001b[0m     \"\"\"\n\u001b[1;32m    238\u001b[0m     y_type, y_true, y_pred, multioutput = _check_reg_targets(\n\u001b[0;32m--> 239\u001b[0;31m         y_true, y_pred, multioutput)\n\u001b[0m\u001b[1;32m    240\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m     output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.6/lib/python3.6/site-packages/sklearn/metrics/regression.py\u001b[0m in \u001b[0;36m_check_reg_targets\u001b[0;34m(y_true, y_pred, multioutput)\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.6/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m             _assert_all_finite(array,\n\u001b[0;32m--> 573\u001b[0;31m                                allow_nan=force_all_finite == 'allow-nan')\n\u001b[0m\u001b[1;32m    574\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m     \u001b[0mshape_repr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_shape_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.6/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan)\u001b[0m\n\u001b[1;32m     54\u001b[0m                 not allow_nan and not np.isfinite(X).all()):\n\u001b[1;32m     55\u001b[0m             \u001b[0mtype_err\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'infinity'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mallow_nan\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'NaN, infinity'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg_err\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float32')."
     ]
    }
   ],
   "source": [
    "folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=1000) #4590\n",
    "oof = np.zeros(len(df_train))\n",
    "predictions = np.zeros(len(df_test))\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(df_train,df_train['outliers'].values)):\n",
    "    print(\"Fold {}\".format(fold_))\n",
    "    x_train, x_val = df_train[df_train_columns].iloc[trn_idx], df_train[df_train_columns].iloc[val_idx]\n",
    "    y_train, y_val = target.iloc[trn_idx], target.iloc[val_idx]\n",
    "\n",
    "    batch_no = x_train.shape[0] // batch_size \n",
    "    \n",
    "# NN ==========================================================================\n",
    "    for epoch in range(num_epochs):\n",
    "            \n",
    "        x_train2, y_train2 = shuffle(x_train, y_train, random_state=epoch)\n",
    "        \n",
    "        net.train()\n",
    "        for i in range(batch_no):\n",
    "            start = i * batch_size\n",
    "            end = start + batch_size\n",
    "            x_var = Variable(torch.FloatTensor(x_train2.values[start:end]))\n",
    "            t_var = Variable(torch.FloatTensor(y_train2.values[start:end]))\n",
    "            \n",
    "            # Forward + Backward + Optimize\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(x_var)\n",
    "            loss = criterion(outputs, t_var)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        #評価\n",
    "        net.eval()\n",
    "        x_val_var = Variable(torch.FloatTensor(x_val.values), requires_grad=True)\n",
    "        x_train_var = Variable(torch.FloatTensor(x_train.values), requires_grad=True)\n",
    "        with torch.no_grad():\n",
    "            x_train_outputs = net(x_train_var).numpy().reshape(x_train.shape[0])\n",
    "            x_val_outputs = net(x_val_var).numpy().reshape(x_val.shape[0])\n",
    "        train_rmse = np.sqrt(mean_squared_error(np.log2(y_train), np.log2(x_train_outputs)))\n",
    "        val_rmse = np.sqrt(mean_squared_error(np.log2(y_val), np.log2(x_val_outputs)))\n",
    "        \n",
    "        print(\"[%d]     training's rmse: %.4f     valid_1's rmse: %.4f\" %(epoch+1, train_rmse, val_rmse))\n",
    "        \n",
    "    \n",
    "    x_val_var = Variable(torch.FloatTensor(x_val.values), requires_grad=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = net(x_val_var).numpy().reshape(x_val.shape[0])\n",
    "    oof[val_idx] = outputs\n",
    "    \n",
    "    rmse = np.sqrt(mean_squared_error(np.log2(outputs),np.log2( y_val.values)))\n",
    "    print(\"valid's_loss: %.4f\" %rmse)\n",
    "    \n",
    "    x_test_var = Variable(torch.FloatTensor(df_test[df_train_columns].values), requires_grad=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = net(x_test_var)\n",
    "    predictions += np.log2(outputs.numpy().reshape(df_test.shape[0])) / folds.n_splits\n",
    "# NN ==========================================================================\n",
    "\n",
    "print(np.sqrt(mean_squared_error(np.log2(target), np.log2(oof))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
